# CausalKnowledgeTrace: Interactive DAG Visualization and Causal Knowledge Graph Generation

CausalKnowledgeTrace is a comprehensive system for interactive visualization of Directed Acyclic Graphs (DAGs) and automated causal knowledge graph generation from biomedical literature. The system integrates a Shiny web application for visualization and configuration with a Python-based graph creation engine that queries SemMedDB for causal relationships.

## ğŸ“‹ What This Project Does

- **ğŸŒ Interactive Visualization**: Web-based DAG exploration with zoom, pan, and node interaction
- **ğŸ” Graphical Causal Modeling**: Automated assembly of causal relationships from biomedical literature given Concept Unique Identifiers in the Unified Medical Language System, or [UMLS](https://www.nlm.nih.gov/research/umls/index.html), for the Exposure and Outcome of interest
- **ğŸ“Š Evidence Analysis**: PMID-based evidence tracking and strength assessment
- **âš¡ Performance Optimized**: Binary formats, caching, and vectorized operations for large graphs
- **ğŸ¯ Configurable Analysis**: Enter multiple CUIs for the exposure and/or outcome; Examine 1st, 2nd, or 3rd degree relationships
- **ğŸ“ Multiple Formats**: R DAG files, JSON assertions, optimized binary formats

## Key Features

### ğŸŒ Shiny Web Application

- **Interactive Network Visualization**: Explore DAGs with zoom, pan, and node selection capabilities
- **Dynamic Node Information**: Click on nodes to see detailed information and evidence
- **Physics Controls**: Adjust network layout parameters in real-time
- **Statistics Dashboard**: View network statistics and node distributions
- **Color-coded Categories**: Three-category system (Exposure/Outcome/Other) with optimized performance
- **Flexible Data Loading**: Load DAG structures from generated files or upload custom R files
- **Graph Configuration Interface**: Configure parameters for knowledge graph generation
- **Efficient Loading**: Fast loading for large graphs

### ğŸ Graph Creation Engine

- **Automated Knowledge Graph Generation**: Create causal graphs from SemMedDB biomedical literature
- **Multiple CUI Support**: Handle multiple Concept Unique Identifiers for exposures and outcomes
- **K-hop Analysis**: Configurable relationship depth (1-3 hops) for comprehensive graph traversal
- **Markov Blanket Analysis**: Advanced causal inference with Markov blanket computation
- **Blacklist Filtering**: Filter out generic or unwanted concepts during graph creation
- **Multiple Output Formats**: Generate R DAG objects, JSON assertion files, and optimized binary formats
- **Performance Monitoring**: Detailed timing analysis and execution metrics

## Project Structure

The project is organized into two main components with clear separation of concerns:

```
CausalKnowledgeTrace/
â”œâ”€â”€ README.md                    # This documentation file
â”œâ”€â”€ setup.sh                    # Automated setup script using conda environment
â”œâ”€â”€ run_app.R                    # Enhanced launch script for Shiny application
â”œâ”€â”€ user_input.yaml              # Configuration file (generated by Shiny app)
â”œâ”€â”€ packages.R                   # R package installation script
â”œâ”€â”€ .env                         # Database credentials (create from doc/smaple.env)
â”‚
â”œâ”€â”€ doc/                         # Setup and configuration files
â”‚   â”œâ”€â”€ environment.yaml         # Conda environment specification
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â”œâ”€â”€ packages.R               # R package installation script
â”‚   â”œâ”€â”€ smaple.env               # Sample environment variables template
â”‚   â””â”€â”€ filter.sql               # Database filtering queries for generic CUIs
â”‚
â”œâ”€â”€ shiny_app/                   # Shiny Web Application Component
â”‚   â”œâ”€â”€ app.R                    # Main Shiny application file
â”‚   â”œâ”€â”€ dag_data.R               # DAG data configuration file
â”‚   â”œâ”€â”€ modules/                 # Modular Shiny components
â”‚   â”‚   â”œâ”€â”€ dag_visualization.R  # Network visualization module
â”‚   â”‚   â”œâ”€â”€ node_information.R   # Node information display module
â”‚   â”‚   â”œâ”€â”€ statistics.R         # Statistics and analytics module
â”‚   â”‚   â”œâ”€â”€ data_upload.R        # Data loading and file management
â”‚   â”‚   â”œâ”€â”€ causal_analysis.R    # Causal analysis functionality
â”‚   â”‚   â””â”€â”€ graph_cache.R        # Graph caching system
â”‚   â”œâ”€â”€ server/                  # Server-side logic modules
â”‚   â”œâ”€â”€ ui/                      # UI component modules
â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚
â””â”€â”€ graph_creation/              # Graph Creation Engine Component
    â”œâ”€â”€ pushkin.py               # Main entry point (delegates to cli_interface.py)
    â”œâ”€â”€ cli_interface.py         # Command line interface and argument parsing
    â”œâ”€â”€ analysis_core.py         # Core analysis classes (GraphAnalyzer, MarkovBlanketAnalyzer)
    â”œâ”€â”€ config_models.py         # Configuration models and validation
    â”œâ”€â”€ database_operations.py   # Database connection and query operations
    â”œâ”€â”€ graph_operations.py      # Graph construction and manipulation
    â”œâ”€â”€ markov_blanket.py        # Markov blanket computation algorithms
    â”œâ”€â”€ post_process_optimization.py # File optimization (disabled)
    â”œâ”€â”€ config.py                # Backward compatibility wrapper
    â”œâ”€â”€ consolidation.py         # Graph consolidation utilities
    â”œâ”€â”€ SemDAGconsolidator.py    # SemMedDB DAG consolidation
    â”œâ”€â”€ example/                 # Example scripts and configurations
    â”‚   â”œâ”€â”€ run_pushkin.sh       # Complete pipeline execution script
    â”‚   â””â”€â”€ run_consolidation.sh # Consolidation-only script
    â”œâ”€â”€ result/                  # Generated output files
    â”‚   â”œâ”€â”€ MarkovBlanket_Union.R
    â”‚   â”œâ”€â”€ degree_X.R           # X = K-hops value (1, 2, or 3)
    â”‚   â”œâ”€â”€ causal_assertions_X.json
    â”‚   â””â”€â”€ performance_metrics.json
    â””â”€â”€ output/                  # Alternative output directory
```

## Prerequisites

### System Requirements

- **PostgreSQL** database with modified SemMEDdb data (for graph creation) in postgres
- **Conda/Miniconda**: For environment management (required)
- **Python** (version 3.11 or higher)
- **R** (version 4.0 or higher): Download from [https://cran.r-project.org/](https://cran.r-project.org/)

### Database Requirements

- PostgreSQL database with SemMedDB schema
- Database connection parameters (host, port, username, password, database name)
- Properly configured `causalehr` schema with causalpredication table

## Installation

### Step 1: PostgreSQL Database Setup for SemMedDB

#### Database Download

We created a modified version of [SemMedDB](https://skr3.nlm.nih.gov/SemMedDB/) that is available in PostgreSQL format.

**Download Link**: https://drive.google.com/file/d/1842FzR1viGkKU3IdqMpBlD2GnupuGF1n/view?usp=drive_link

**Note**: The database file is approximately 25GB in size, so the download may take several minutes depending on your internet connection.

#### Database Setup Instructions

**Prerequisites:**

- PostgreSQL must be installed on your system
- Sufficient disk space (at least 50GB recommended for extraction and setup)

**Create the Database:**
Replace `<username>` with your actual PostgreSQL username.

```bash
createdb -U <username> -h localhost causalehr
```

### Step 2: Install Miniconda

This project requires Conda for environment management. If you don't have Conda/Miniconda installed:

#### Download Miniconda

- **Windows**: [Miniconda3 Windows 64-bit](https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe)
- **macOS**: [Miniconda3 macOS 64-bit](https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh)
- **macOS (Apple Silicon)**: [Miniconda3 macOS ARM64](https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh)
- **Linux**: [Miniconda3 Linux 64-bit](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh)

#### Install Miniconda

**Windows:**

1. Download the Windows installer from the link above
2. Run the `.exe` file as administrator
3. Follow the installation wizard
4. Check "Add Anaconda to my PATH environment variable" (optional but recommended)
5. Complete installation and restart your command prompt

**macOS/Linux:**

1. Download the appropriate `.sh` file for your system
2. Open Terminal and navigate to the download location
3. Run the installer:
   ```bash
   bash Miniconda3-latest-[YourSystem].sh
   ```
4. Follow the prompts and accept the license
5. Allow the installer to initialize conda
6. Restart your terminal or run: `source ~/.bashrc`

#### Verify Conda Installation

Check if conda is installed correctly:

```bash
conda --version
conda info
```

### Step 3: Project Setup

The project includes an automated setup script that creates a conda environment and installs all dependencies:

```bash
# Clone or download the project
cd CausalKnowledgeTrace

# 1. Create environment configuration file
cp doc/smaple.env .env
# Edit .env with your database credentials (see Database Configuration below)

# 2. Run the automated setup script
chmod +x setup.sh
./setup.sh
```

This script will:

1. Create a conda environment named `causalknowledgetrace` using `doc/environment.yaml`
2. Install Python dependencies (psycopg2, PyYAML, pandas, networkx, scipy, numpy, matplotlib, jupyter, rpy2)
3. Install R packages using `doc/packages.R`

### Database Configuration

Before running graph creation, you need to configure your database connection:

```bash
# Copy the sample environment file
cp doc/smaple.env .env

# Edit the .env file with your database credentials
nano .env  # or use your preferred editor
```

The `.env` file should contain:

```bash
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_USER=your_username
DB_PASSWORD=your_password
DB_NAME=your_database_name
DB_SCHEMA=causalehr
```

**Important**:

- Replace the placeholder values with your actual database credentials
- The `.env` file is ignored by git for security (contains sensitive information)
- Make sure your database contains the SemMedDB schema with causalpredication table

### Manual Installation (Alternative)

If you prefer manual installation or the automated setup fails:

#### 1. Setup Environment Configuration

```bash
# Copy the sample environment file
cp doc/smaple.env .env

# Edit with your database credentials
nano .env  # or use your preferred editor
```

#### 2. Create Conda Environment

```bash
# Create environment from YAML file
conda env create -f doc/environment.yaml

# Activate the environment
conda activate causalknowledgetrace
```

#### 3. Install Python Dependencies

```bash
# Using pip with requirements file
pip install -r doc/requirements.txt

# Or install individually
pip install psycopg2-binary PyYAML pandas networkx scipy rpy2
```

#### 4. Install R Packages

```r
# Run the R package installation script
source("doc/packages.R")

# Or install core packages manually
install.packages(c(
    "shiny", "shinydashboard", "visNetwork", "dplyr", "DT",
    "dagitty", "igraph", "yaml", "shinyjs", "SEMgraph",
    "ggplot2", "testthat", "knitr", "rmarkdown"
))
```

## Configuration and Workflow

### Typical User Workflow (Recommended)

For most users, the integrated web application provides the complete workflow:

1. **Launch Application**: `Rscript run_app.R`
2. **Configure Analysis**: Use "Graph Configuration" tab to set parameters
3. **Generate Graphs**: Click "Create Graph" to generate directly in the app
4. **Visualize Results**: Automatically load and explore generated graphs
5. **Interactive Analysis**: Use all visualization and analysis features

### YAML Configuration Format

The `user_input.yaml` file supports the following structure:

```yaml
# Multiple CUIs supported for both exposure and outcome
exposure_cuis:
  - C0020538  # Hypertension
  - C4013784  # Hypertensive disease
  - C0221155  # High blood pressure
outcome_cuis:
  - C2677888  # Alzheimer's disease
  - C0750901  # Dementia
  - C0494463  # Cognitive impairment

# Optional blacklist to exclude generic concepts
blacklist_cuis:
  - C0001687  # Generic concept to exclude
  - C0002526  # Another generic concept

# Custom names (optional)
exposure_name: "Hypertension"
outcome_name: "Alzheimers"

# Analysis parameters
min_pmids: 10                    # Minimum publications required
pub_year_cutoff: 2010           # Exclude publications before 2010
k_hops: 1                       # Relationship depth (1-3)
predication_type: "CAUSES"      # Single type or list
SemMedDBD_version: "heuristic"  # Database version
```

### Predication Types

The system supports multiple predication types:

- **Single type**: `predication_type: "CAUSES"`
- **Multiple types**: `predication_type: ["CAUSES", "TREATS", "PREVENTS"]`

Common types: `CAUSES`, `TREATS`, `PREVENTS`, `INTERACTS_WITH`, `AFFECTS`, `ASSOCIATED_WITH`, `PREDISPOSES`, `COMPLICATES`

## Data Loading and Visualization

### Loading Generated Graphs

The application provides optimized loading for graphs generated by the Python engine:

#### Method 1: Generated Graph Files

1. **Generate graphs** using the graph creation engine
2. **Files are saved** to `graph_creation/result/` directory:

   - `degree_X.R` - Standard DAG files (X = k-hops value)
   - `MarkovBlanket_Union.R` - Markov blanket analysis results
   - `causal_assertions_X.json` - Evidence data
3. **Load in Shiny app**:

   - Go to "Data Upload" tab
   - Click "Refresh File List" to scan for available files
   - Select your generated file from the dropdown
   - Click "Load Selected DAG"

#### Method 2: Upload Custom DAG Files

1. **Go to "Data Upload" tab** in the Shiny app
2. **Use file upload interface** to select your R file
3. **Click "Upload & Load"** to upload and immediately load the DAG

### DAG File Format

DAG files should contain a dagitty graph definition with variable name `g`:

```r
# Example DAG file format
g <- dagitty('dag {
    Hypertension [exposure]
    Alzheimers_Disease [outcome]
    Age
    BMI
    Diabetes

    Age -> Hypertension
    BMI -> Diabetes
    Hypertension -> Alzheimers_Disease
    Diabetes -> Alzheimers_Disease
    Age -> Alzheimers_Disease
}')
```

### Performance Features

The application includes several performance features:

- **Automatic file detection** and validation
- **Graph caching system** for repeated access
- **Node categorization** using vectorized operations
- **Progressive loading** with status indicators

### Node Categories and Visualization

The application uses an optimized three-category system for node classification:

- **ğŸ”´ Exposure** (Red #FF4500): Variables marked as `[exposure]` in the DAG
- **ğŸ”µ Outcome** (Blue #0066CC): Variables marked as `[outcome]` in the DAG
- **âš« Other** (Gray #808080): All other variables in the DAG

**Performance features:**

- Vectorized operations for categorization of large graphs
- Color assignment using batch processing
- Font sizing and edge styling for better visibility
- Progress indicators during processing

## Advanced Usage

### Graph Creation Options

#### Standard Graph Analysis

```bash
cd graph_creation
python pushkin.py --yaml-config ../user_input.yaml --host localhost --port 5432 --dbname causalehr --user username --password password --schema causalehr
```

### Database Filtering

The project includes SQL filters to exclude generic concepts:

- **Generic CUI filtering**: Removes overly broad medical concepts
- **Semantic type filtering**: Excludes non-specific semantic types
- **Custom blacklists**: User-defined CUI exclusions

Filter file: `doc/filter.sql` contains predefined exclusion lists

## Shiny Application Usage

### Application Tabs

#### 1. âš™ï¸ Graph Configuration Tab

- **Parameter configuration**: Set exposure/outcome CUIs, thresholds, filters
- **Multiple CUI support**: Handle complex exposure-outcome relationships
- **Blacklist management**: Exclude unwanted concepts
- **Real-time validation**: Input validation with immediate feedback
- **YAML export**: Save configuration for Python engine

#### 2. ğŸ“ Data Upload Tab

- **File management**: Upload and load custom DAG files
- **Generated file loading**: Access graphs created by Python engine
- **File format validation**: Automatic dagitty syntax checking
- **Status indicators**: Real-time feedback on loading progress

#### 3. ğŸ“Š DAG Visualization Tab

- **Interactive network diagram** with full DAG structure
- **Node interaction**: Drag to reposition, click to select and view details
- **Zoom and pan**: Mouse wheel zoom, drag to pan
- **Physics controls**: Real-time adjustment of layout parameters
- **Node removal**: Interactive removal of nodes and edges with graph updates
- **Reload DAG Data**: Refresh from current data files
- **Create Graph**: Navigate to graph configuration

#### 4. ğŸ” Causal Analysis Tab

- **Adjustment set identification**: Find minimal sufficient adjustment sets
- **Instrumental variable detection**: Identify valid instruments
- **Causal path analysis**: Explore all paths between exposure and outcome
- **Interactive causal inference**: Real-time analysis with DAG updates

#### 5. ğŸ“‹ Node Information Tab

- **Selected node details**: Comprehensive information about clicked nodes
- **Searchable node table**: All nodes with metadata and properties
- **Evidence information**: PMID lists and evidence counts (for generated graphs)
- **Instrumental variables**: Displayed as comma-separated lists

#### 6. ğŸ“ˆ Statistics Tab

- **Network metrics**: Node count, edge count, group distributions
- **Visual charts**: Node distribution and category breakdowns
- **DAG structure**: Connectivity and topology information
- **Performance metrics**: Loading times and processing status

## Technical Details

### Graph Creation Engine Architecture

#### Core Components

- **`pushkin.py`**: Main entry point that delegates to CLI interface
- **`cli_interface.py`**: Command-line argument parsing and validation
- **`analysis_core.py`**: Core analyzer classes:
  - `GraphAnalyzer`: General graph analysis and DAG generation
  - `MarkovBlanketAnalyzer`: Specialized Markov blanket computation
- **`config_models.py`**: Configuration validation and data models
- **`database_operations.py`**: PostgreSQL database operations and k-hop queries
- **`markov_blanket.py`**: Markov blanket algorithms for causal inference

#### Analysis Modes

1. **Standard Graph Analysis**: Basic causal graph construction from SemMedDB
2. **Markov Blanket Analysis**: Advanced causal inference with confounder identification
3. **K-hop Analysis**: Configurable relationship depth (1-3 hops) for comprehensive traversal

#### Output Formats

- **R DAG files**: `degree_X.R`, `MarkovBlanket_Union.R` (dagitty format)
- **JSON assertions**: `causal_assertions_X.json` (detailed evidence)
- **Metadata**: Performance metrics, configuration logs, timing analysis

### Troubleshooting

### Setup Issues

#### Conda Environment Setup

```bash
# If conda command not found (Linux/macOS)
export PATH="/path/to/miniconda3/bin:$PATH"
# Or reinitialize
conda init bash  # or zsh, fish, etc.

# If conda environment creation fails
conda clean --all
conda env create -f doc/environment.yaml --force

# If R package installation fails
Rscript -e "install.packages('devtools'); devtools::install_deps()"
```

#### Database Connection

```bash
# Test PostgreSQL connection
psql -h localhost -p 5432 -U username -d causalehr -c "SELECT COUNT(*) FROM causalehr.causalpredication;"

# Check schema exists
psql -h localhost -p 5432 -U username -d causalehr -c "\dt causalehr.*"
```

### **Environment Variable Debugging**:

```bash
# Check if environment variables are loaded correctly
echo "DB_HOST: $DB_HOST"
echo "DB_PORT: $DB_PORT"
echo "DB_USER: $DB_USER"
echo "DB_NAME: $DB_NAME"
```

**Note**: The Python code automatically handles string-to-integer conversion for the port parameter, so both `DB_PORT=5432` and `DB_PORT="5432"` work correctly.

### Performance Tips

- Check for generated files in `graph_creation/result/` directory
- Monitor console output for processing status
- Clear cache if needed: `rm -rf shiny_app/.cache/`

### Data Validation

The system includes comprehensive validation:

- **YAML configuration**: Real-time validation with error messages
- **DAG file format**: Automatic dagitty syntax checking
- **Database queries**: Parameter validation and SQL injection prevention
- **File integrity**: Checksum validation for optimized files
