# CausalKnowledgeTrace: Interactive Literature-Based Causal Structure Mapping, Graph Generation, Visualization, and Refinement

CausalKnowledgeTrace (CKT) automates causal knowledge graph generation from biomedical literature. The system extracts causal relationships from SemMedDB, a database of semantic predications derived from PubMed abstracts.

CKT has two main components. A Python engine handles graph construction and causal identification algorithms. A Shiny web application provides interactive visualization and user configuration.

Users specify an exposure and outcome of interest. They can constrain the search by publication year, causal predicate type, and minimum number of supporting articles per relationship. CKT constructs initial partially directed acyclic graphs (PDAGs) representing causal structures between biomedical concepts. Users then edit these graphs interactively to remove unnecessary nodes and edges. CKT can export graphs and evidence from the literature for downstream analysis.

Additional tools in the furtherAnalysis folder refine the exported graphs. These tools classify variables by their causal role relative to the exposure and outcome. They identify nodes within three causal steps of the exposure or outcome. The tools also remove extraneous variables and detect minimal adjustment sets for unbiased effect estimation. This includes adjustment sets that address butterfly bias, a form of sample selection bias in causal graphs.

The adjustment sets account for confounding while avoiding collider bias and selection bias. This architecture supports systematic causal inference in observational biomedical research. Researchers can explore alternative causal structures and test different assumptions about edge directionality. The tool facilitates hypothesis generation and study design for epidemiological analyses.

Usage instructions (*a veritable work-in-progress*) are available here: [Usage Instructions](https://docs.google.com/document/d/1SOr5PCclzzkw6_R13Swf0NEyNDwJL9FUW2pQY6wafSs/edit?usp=sharing)

## ğŸ“‹ What This Project Does

- **ğŸŒ Interactive Visualization**: Web-based DAG exploration with zoom, pan, and node interaction
- **ğŸ” Graphical Causal Modeling**: Automated assembly of causal relationships from biomedical literature given Concept Unique Identifiers in the Unified Medical Language System, or [UMLS](https://www.nlm.nih.gov/research/umls/index.html), for the Exposure and Outcome of interest
- **ğŸ“Š Evidence Analysis**: PMID-based evidence tracking and strength assessment
- **âš¡ Performance Optimized**: Binary formats, caching, and vectorized operations for large graphs
- **ğŸ¯ Configurable Analysis**: Enter multiple CUIs for the exposure and/or outcome; Examine 1st, 2nd, or 3rd degree relationships
- **ğŸ“ Multiple Formats**: R DAG files, JSON assertions, optimized binary formats

## Key Features

### ğŸŒ Shiny Web Application

- **Interactive Network Visualization**: Explore DAGs with zoom, pan, and node selection capabilities
- **Dynamic Node Information**: Click on nodes to see detailed information and evidence
- **Physics Controls**: Adjust network layout parameters in real-time
- **Statistics Dashboard**: View network statistics and node distributions
- **Color-coded Categories**: Three-category system (Exposure/Outcome/Other) with optimized performance
- **Flexible Data Loading**: Load DAG structures from generated files or upload custom R files
- **Graph Configuration Interface**: Configure parameters for knowledge graph generation
- **Enhanced CUI Search**: Searchable interface for medical concept selection with semantic type information
- **Efficient Loading**: Fast loading for large graphs

### ğŸ Graph Creation Engine

- **Automated Knowledge Graph Generation**: Create causal graphs from SemMedDB biomedical literature
- **Multiple CUI Support**: Handle multiple Concept Unique Identifiers for exposures and outcomes
- **K-hop Analysis**: Configurable relationship depth (1-3 hops) for comprehensive graph traversal
- **Markov Blanket Analysis**: Advanced causal inference with Markov blanket computation
- **Blacklist Filtering**: Filter out generic or unwanted concepts during graph creation
- **Multiple Output Formats**: Generate R DAG objects, JSON assertion files, and optimized binary formats
- **Performance Monitoring**: Detailed timing analysis and execution metrics

## Project Structure

The project is organized into two main components with clear separation of concerns:

```
CausalKnowledgeTrace/
â”œâ”€â”€ README.md                    # This documentation file
â”œâ”€â”€ docker-compose.yaml          # Docker Compose configuration
â”œâ”€â”€ restore.sh                   # Database restoration script (for Docker)
â”œâ”€â”€ run_app.R                    # Enhanced launch script for Shiny application
â”œâ”€â”€ user_input.yaml              # Configuration file (generated by Shiny app)
â”œâ”€â”€ .env                         # Database credentials (create from doc/sample.env)
â”‚
â”œâ”€â”€ docker/                      # Docker configuration
â”‚   â””â”€â”€ Dockerfile               # Application container definition
â”‚
â”œâ”€â”€ doc/                         # Setup and configuration files
â”‚   â”œâ”€â”€ DOCKER_INSTALLATION.md   # Docker installation guide
â”‚   â”œâ”€â”€ MANUAL_INSTALLATION.md   # Manual installation guide
â”‚   â”œâ”€â”€ environment.yaml         # Conda environment specification
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â”œâ”€â”€ packages.R               # R package installation script
â”‚   â”œâ”€â”€ sample.env               # Sample environment variables template
â”‚   â”œâ”€â”€ filter.sql               # Database filtering queries for generic CUIs
â”‚   â””â”€â”€ create_cui_search_table.sql # CUI search index table creation script
â”‚
â”œâ”€â”€ shiny_app/                   # Shiny Web Application Component
â”‚   â”œâ”€â”€ app.R                    # Main Shiny application file
â”‚   â”œâ”€â”€ dag_data.R               # DAG data configuration file
â”‚   â”œâ”€â”€ modules/                 # Modular Shiny components
â”‚   â”‚   â”œâ”€â”€ dag_visualization.R  # Network visualization module
â”‚   â”‚   â”œâ”€â”€ node_information.R   # Node information display module
â”‚   â”‚   â”œâ”€â”€ statistics.R         # Statistics and analytics module
â”‚   â”‚   â”œâ”€â”€ data_upload.R        # Data loading and file management
â”‚   â”‚   â”œâ”€â”€ causal_analysis.R    # Causal analysis functionality
â”‚   â”‚   â””â”€â”€ graph_cache.R        # Graph caching system
â”‚   â”œâ”€â”€ server/                  # Server-side logic modules
â”‚   â”œâ”€â”€ ui/                      # UI component modules
â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚
â”œâ”€â”€ furtherAnalysis/             # Analytic code and features that haven't been worked into the main body of code
â”‚   â”œâ”€â”€ classifyVariables.R      # Classifies each variable by its causal role, e.g., confounder, collider, mediator, IV, and the like.
â”‚   â”œâ”€â”€ cleanByRemovingLeaves.R  # Removes leaves (singly-connected nodes) from the causal graph from initial search
â”‚   â”œâ”€â”€ cleanButterflyReport-CausalGraph.R    # code for identifying minimally sufficient adjustment sets in context of butterfly bias 
â”‚   â”œâ”€â”€ cleanButterflyReport-CausalGraph_compiled.R #compiled version of above
â”‚   â”œâ”€â”€ mBiasReport-CausalGraph.R# identifying variables implicated in M-bias from causal graph
â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚
â””â”€â”€ graph_creation/              # Graph Creation Engine Component
    â”œâ”€â”€ pushkin.py               # Main entry point (delegates to cli_interface.py)
    â”œâ”€â”€ cli_interface.py         # Command line interface and argument parsing
    â”œâ”€â”€ analysis_core.py         # Core analysis classes (GraphAnalyzer, MarkovBlanketAnalyzer)
    â”œâ”€â”€ config_models.py         # Configuration models and validation
    â”œâ”€â”€ database_operations.py   # Database connection and query operations
    â”œâ”€â”€ graph_operations.py      # Graph construction and manipulation
    â”œâ”€â”€ markov_blanket.py        # Markov blanket computation algorithms
    â”œâ”€â”€ post_process_optimization.py # File optimization (disabled)
    â”œâ”€â”€ config.py                # Backward compatibility wrapper
    â”œâ”€â”€ consolidation.py         # Graph consolidation utilities
    â”œâ”€â”€ SemDAGconsolidator.py    # SemMedDB DAG consolidation
    â”œâ”€â”€ example/                 # Example scripts and configurations
    â”‚   â”œâ”€â”€ run_pushkin.sh       # Complete pipeline execution script
    â”‚   â””â”€â”€ run_consolidation.sh # Consolidation-only script
    â”œâ”€â”€ result/                  # Generated output files
    â”‚   â”œâ”€â”€ MarkovBlanket_Union.R
    â”‚   â”œâ”€â”€ degree_X.R           # X = K-hops value (1, 2, or 3)
    â”‚   â”œâ”€â”€ causal_assertions_X.json
    â”‚   â””â”€â”€ performance_metrics.json
    â””â”€â”€ output/                  # Alternative output directory
```

## Prerequisites

### System Requirements

- **Disk Space**: At least 50GB free (for database and dependencies)
- **RAM**: 8GB minimum, 16GB recommended
- **Operating System**: Linux, macOS, or Windows

### Database Requirements

- Modified SemMedDB database (PostgreSQL format)
- Database backup file (~25GB download)
- **CUI Search Index Table**: Automatically created by the application on first use

## Installation

### Common Setup Steps (Required for Both Methods)

Before choosing your installation method, complete these common steps:

#### Step 1: Get the Repository

**Option A: Clone with Git (Recommended)**

Git allows you to easily pull future updates to the project.

```bash
# Install Git if needed
# Linux: sudo apt-get install git
# macOS: brew install git
# Windows: https://git-scm.com/download/win

# Verify Git installation
git --version

# Clone the repository
git clone git@github.com:unmtransinfo/CausalKnowledgeTrace.git
cd CausalKnowledgeTrace

# To get future updates later:
# git pull origin main
```

**Option B: Download as ZIP**

If you don't want to install Git:

1. Download: [Download ZIP from GitHub](https://github.com/unmtransinfo/CausalKnowledgeTrace/archive/refs/heads/main.zip)
2. Extract the ZIP file
3. Open terminal/command prompt and navigate to the extracted directory

#### Step 2: Download Database Backup

Download the SemMedDB database backup file:

**Download Link**: [causalehr_backup.tar.gz from OneDrive](https://unmm-my.sharepoint.com/:u:/g/personal/rajeshupadhayaya_unm_edu/ESO2UPECVk5Ku3JRSClPytMBngCV_0QN8-cA-zQRjaYogg?e=YolDZH)

**Note**: The file is approximately 25GB. Download may take several minutes depending on your internet connection.

#### Step 3: Extract Database Backup

Extract the database backup to the project root directory:

```bash
# Navigate to the project directory (if not already there)
cd CausalKnowledgeTrace

# Extract the backup file
tar -xzf causalehr_backup.tar.gz

# Verify the backup directory exists
ls -la causalehr_backup/
```

You should see multiple `.dat.gz` files and a `toc.dat` file in the `causalehr_backup/` directory.

---

### Choose Your Installation Method

Now that you have the repository and database backup, choose your installation method:

### ğŸ³ Docker Installation (Recommended)

**Best for:** Quick setup, testing, and most users

**Time:** ~5 minutes (after common setup)

**Prerequisites:** Docker and Docker Compose only

Docker provides a containerized environment with all dependencies pre-configured. This is the fastest and easiest way to get started.

**ğŸ“– [Complete Docker Installation Guide â†’](doc/DOCKER_INSTALLATION.md)**

**Quick Start:**

```bash
# 1. Configure environment
cp doc/sample.env .env
# Edit .env with your credentials (use DB_HOST=db for Docker)

# 2. Start application
docker-compose up -d

# 3. Access at http://localhost:3838
```

---

### ğŸ”§ Manual Installation

**Best for:** Development, customization, and advanced users

**Time:** ~30 minutes (after common setup)

**Prerequisites:** PostgreSQL, Conda, Python 3.11+, R 4.0+

Manual installation gives you full control over the environment and is recommended for development and production deployments.

**ğŸ“– [Complete Manual Installation Guide â†’](doc/MANUAL_INSTALLATION.md)**

**Quick Start:**

```bash
# 1. Setup database
createdb -U <username> causalehr
pg_restore -d causalehr -U <username> causalehr_backup/

# 2. Configure environment
cp doc/sample.env .env
# Edit .env with your credentials (use DB_HOST=localhost)

# 3. Setup conda environment
conda env create -f doc/environment.yaml
conda activate causalknowledgetrace
pip install -r doc/requirements.txt

# 4. Install R packages
Rscript doc/packages.R

# 5. Run application
Rscript run_app.R
```

---


